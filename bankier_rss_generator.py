#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Generator kana≈Çu RSS dla Bankier.pl/wiadomosc
Optimized for GitHub Actions & automation
"""

import requests
from bs4 import BeautifulSoup
from feedgen.feed import FeedGenerator
from datetime import datetime, timedelta
import pytz
import time
import sys
from urllib.parse import urljoin

# ============================================================================
# KONFIGURACJA
# ============================================================================

BASE_URL = "https://www.bankier.pl"
NEWS_URL = f"{BASE_URL}/wiadomosc/"
PAGES_TO_SCAN = 5  # Liczba stron do przeskanowania
TIME_FILTER_HOURS = 48  # Artyku≈Çy z ostatnich 48h
OUTPUT_FILE = "bankier_rss.xml"
REQUEST_DELAY = 2  # Sekundy miƒôdzy requestami (anti-bot)

# Nag≈Ç√≥wki HTTP imitujƒÖce przeglƒÖdarkƒô
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'pl-PL,pl;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'Referer': 'https://www.bankier.pl/',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Cache-Control': 'max-age=0',
}

# ============================================================================
# FUNKCJE POMOCNICZE
# ============================================================================

def get_page_url(page_number):
    """Generuje URL dla danej strony"""
    if page_number == 1:
        return NEWS_URL
    return f"{NEWS_URL}{page_number}/"


def fetch_page(url, retry=3):
    """Pobiera stronƒô z obs≈ÇugƒÖ b≈Çƒôd√≥w i retry"""
    for attempt in range(retry):
        try:
            print(f"  ‚Üí Pobieranie: {url} (pr√≥ba {attempt + 1}/{retry})")
            response = requests.get(url, headers=HEADERS, timeout=15, allow_redirects=True)
            response.raise_for_status()
            # POPRAWKA: Wymuszamy UTF-8
            response.encoding = 'utf-8'
            print(f"  ‚úì Sukces: {response.status_code} ({len(response.content)} bajt√≥w)")
            return response
        except requests.RequestException as e:
            print(f"  ‚úó B≈ÇƒÖd: {e}")
            if attempt < retry - 1:
                wait_time = (attempt + 1) * 2
                print(f"  ‚è≥ Ponowna pr√≥ba za {wait_time}s...")
                time.sleep(wait_time)
            else:
                print(f"  ‚úó Nie uda≈Ço siƒô pobraƒá strony po {retry} pr√≥bach")
                return None


def parse_datetime(datetime_str):
    """Parsuje datƒô w formacie ISO 8601 z timezone"""
    try:
        # Format: 2025-12-30T11:44:00+01:00
        dt = datetime.fromisoformat(datetime_str)
        # Upewniamy siƒô, ≈ºe ma timezone
        if dt.tzinfo is None:
            dt = pytz.timezone('Europe/Warsaw').localize(dt)
        return dt
    except Exception as e:
        print(f"  ‚ö† B≈ÇƒÖd parsowania daty '{datetime_str}': {e}")
        return None


def is_recent(article_date, hours=TIME_FILTER_HOURS):
    """Sprawdza czy artyku≈Ç jest z ostatnich X godzin"""
    if article_date is None:
        return False
    now = datetime.now(pytz.timezone('Europe/Warsaw'))
    cutoff = now - timedelta(hours=hours)
    return article_date >= cutoff


def extract_articles_from_page(soup, page_num):
    """WyciƒÖga artyku≈Çy z pojedynczej strony"""
    articles = []
    
    # Szukamy div√≥w z klasƒÖ "article"
    article_divs = soup.find_all('div', class_='article')
    
    print(f"  üìÑ Znaleziono {len(article_divs)} kontener√≥w <div class='article'> na stronie {page_num}")
    
    for idx, article_div in enumerate(article_divs, 1):
        try:
            # POPRAWKA: Tytu≈Ç jest w <span class="entry-title"> -> <a>
            title_span = article_div.find('span', class_='entry-title')
            
            if not title_span:
                print(f"    ‚ö† [{idx}] Brak <span class='entry-title'> - pomijam")
                continue
            
            title_link = title_span.find('a')
            
            if not title_link or not title_link.get('href'):
                print(f"    ‚ö† [{idx}] Brak linku w entry-title - pomijam")
                continue
            
            if not title_link or not title_link.get('href'):
                print(f"    ‚ö† [{idx}] Brak linku w entry-title - pomijam")
                continue
            
            title = title_link.get_text(strip=True)
            link = title_link.get('href')
            
            # Budujemy pe≈Çny URL
            if not link.startswith('http'):
                link = urljoin(BASE_URL, link)
            
            # Pomijamy linki zewnƒôtrzne/nieprawid≈Çowe
            if not link.startswith(BASE_URL):
                print(f"    ‚ö† [{idx}] Link zewnƒôtrzny - pomijam: {link}")
                continue
            
            # WyciƒÖgamy datƒô z <time class="entry-date"> (PIERWSZY tag time)
            entry_meta = article_div.find('div', class_='entry-meta')
            time_tag = None
            
            if entry_meta:
                time_tag = entry_meta.find('time', class_='entry-date')
            
            # Fallback - szukamy bezpo≈õrednio w article_div
            if not time_tag:
                time_tag = article_div.find('time', class_='entry-date')
            
            if not time_tag or not time_tag.get('datetime'):
                print(f"    ‚ö† [{idx}] Brak daty - pomijam: {title[:50]}...")
                continue
            
            pub_date = parse_datetime(time_tag['datetime'])
            if not pub_date:
                continue
            
            # Filtr czasowy
            if not is_recent(pub_date):
                print(f"    ‚è≠ [{idx}] Za stary artyku≈Ç ({pub_date.strftime('%Y-%m-%d %H:%M')}) - pomijam")
                continue
            
            # WyciƒÖgamy opis z <p> (pierwszy akapit po entry-title)
            description = ""
            # Szukamy <p> w entry-content (pomijamy linki "Czytaj dalej")
            entry_content = article_div.find('div', class_='entry-content')
            if entry_content:
                p_tag = entry_content.find('p')
                if p_tag:
                    # Usuwamy link "Czytaj dalej"
                    more_link = p_tag.find('a', class_='more-link')
                    if more_link:
                        more_link.decompose()
                    description = p_tag.get_text(strip=True)
            
            article = {
                'title': title,
                'link': link,
                'description': description or title,  # Fallback do tytu≈Çu
                'pub_date': pub_date,
                'guid': link,  # Unikalny identyfikator
            }
            
            articles.append(article)
            print(f"    ‚úì [{idx}] {title[:60]}... ({pub_date.strftime('%Y-%m-%d %H:%M')})")
            
        except Exception as e:
            print(f"    ‚úó [{idx}] B≈ÇƒÖd parsowania artyku≈Çu: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    return articles


def remove_duplicates(articles):
    """Usuwa duplikaty na podstawie linku (GUID)"""
    seen = set()
    unique = []
    
    for article in articles:
        if article['guid'] not in seen:
            seen.add(article['guid'])
            unique.append(article)
    
    removed = len(articles) - len(unique)
    if removed > 0:
        print(f"\nüîÑ Usuniƒôto {removed} duplikat√≥w")
    
    return unique


def generate_rss_feed(articles, output_file=OUTPUT_FILE):
    """Generuje plik RSS z artyku≈Çami"""
    print(f"\nüìù Generowanie RSS...")
    
    # Inicjalizacja feed generatora
    fg = FeedGenerator()
    fg.id(NEWS_URL)
    fg.title('Bankier.pl - Wiadomo≈õci')
    fg.author({'name': 'Bankier.pl', 'email': 'redakcja@bankier.pl'})
    fg.link(href=NEWS_URL, rel='alternate')
    fg.description('Najnowsze wiadomo≈õci z serwisu Bankier.pl')
    fg.language('pl')
    fg.updated(datetime.now(pytz.timezone('Europe/Warsaw')))
    
    # Sortujemy artyku≈Çy od najnowszych
    articles_sorted = sorted(articles, key=lambda x: x['pub_date'], reverse=True)
    
    # Dodajemy artyku≈Çy do feedu
    for article in articles_sorted:
        fe = fg.add_entry()
        fe.id(article['guid'])
        fe.title(article['title'])
        fe.link(href=article['link'])
        fe.description(article['description'])
        fe.published(article['pub_date'])
        fe.updated(article['pub_date'])
    
    # Zapisujemy do pliku
    fg.rss_file(output_file, pretty=True)
    print(f"‚úì Zapisano do pliku: {output_file}")
    print(f"‚úì Liczba artyku≈Ç√≥w w RSS: {len(articles_sorted)}")


# ============================================================================
# G≈Å√ìWNA LOGIKA
# ============================================================================

def main():
    """G≈Ç√≥wna funkcja programu"""
    print("=" * 70)
    print("üöÄ BANKIER.PL RSS GENERATOR")
    print("=" * 70)
    print(f"üìÖ Filtr czasowy: ostatnie {TIME_FILTER_HOURS}h")
    print(f"üìÑ Stron do przeskanowania: {PAGES_TO_SCAN}")
    print(f"‚è±Ô∏è  Op√≥≈∫nienie miƒôdzy requestami: {REQUEST_DELAY}s")
    print("=" * 70)
    
    all_articles = []
    
    # Pƒôtla po stronach
    for page_num in range(1, PAGES_TO_SCAN + 1):
        print(f"\nüìñ Strona {page_num}/{PAGES_TO_SCAN}")
        print("-" * 70)
        
        url = get_page_url(page_num)
        response = fetch_page(url)
        
        if response is None:
            print(f"  ‚ö† Pomijam stronƒô {page_num} z powodu b≈Çƒôdu")
            continue
        
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = extract_articles_from_page(soup, page_num)
        all_articles.extend(articles)
        
        print(f"  ‚úì Zebrano {len(articles)} artyku≈Ç√≥w ze strony {page_num}")
        
        # Op√≥≈∫nienie przed kolejnym requestem (anti-bot)
        if page_num < PAGES_TO_SCAN:
            print(f"  ‚è≥ Czekam {REQUEST_DELAY}s przed kolejnƒÖ stronƒÖ...")
            time.sleep(REQUEST_DELAY)
    
    # Podsumowanie
    print("\n" + "=" * 70)
    print(f"üìä PODSUMOWANIE")
    print("=" * 70)
    print(f"Zebrano ≈ÇƒÖcznie: {len(all_articles)} artyku≈Ç√≥w")
    
    if not all_articles:
        print("‚ö† Nie znaleziono ≈ºadnych artyku≈Ç√≥w! Sprawd≈∫ konfiguracjƒô.")
        print("\nüí° DEBUGOWANIE - zapisujƒô pierwszƒÖ stronƒô do pliku debug.html")
        try:
            response = requests.get(NEWS_URL, headers=HEADERS, timeout=10)
            with open('debug.html', 'w', encoding='utf-8') as f:
                f.write(response.text)
            print("‚úì Zapisano debug.html - sprawd≈∫ ten plik aby zobaczyƒá strukturƒô HTML")
        except Exception as e:
            print(f"‚úó Nie uda≈Ço siƒô zapisaƒá debug.html: {e}")
        return 1
    
    # Usuwanie duplikat√≥w
    unique_articles = remove_duplicates(all_articles)
    print(f"Po deduplikacji: {len(unique_articles)} unikalnych artyku≈Ç√≥w")
    
    # Generowanie RSS
    generate_rss_feed(unique_articles)
    
    print("\n" + "=" * 70)
    print("‚úÖ ZAKO≈ÉCZONO POMY≈öLNIE")
    print("=" * 70)
    
    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\n‚ö† Przerwano przez u≈ºytkownika")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n‚ùå KRYTYCZNY B≈ÅƒÑD: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
